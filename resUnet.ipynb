{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expired-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, BatchNormalization,Reshape,Conv2DTranspose\n",
    "from tensorflow.keras.layers import MaxPooling2D, Activation,Add,LeakyReLU,Flatten,Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from dataloader import get_ref_num_list, DataLoader\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_visible_devices(gpus[3], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-marks",
   "metadata": {},
   "source": [
    "# 모델 구축 (ResUnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recognized-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inp,filters,strides,kernel_size=(3,3),padding=\"same\"):\n",
    "    conv = layers.Conv2D(filters, kernel_size=kernel_size, padding=padding, strides=strides[0])(inp)\n",
    "    bn_act = bn_act_block(conv)\n",
    "    conv = layers.Conv2D(filters, kernel_size=kernel_size, padding=padding, strides=strides[1])(bn_act)\n",
    "    return conv \n",
    "\n",
    "def bn_act_block(inp):\n",
    "    bn = layers.BatchNormalization()(inp)\n",
    "    act = layers.Activation('relu')(bn)\n",
    "    return act\n",
    "\n",
    "def inp_concat(ref_img,c1,c2):\n",
    "    img_size = 128\n",
    "    c1_layer = layers.RepeatVector(img_size * img_size)(c1)\n",
    "    c1_layer = layers.Reshape([img_size, img_size, 1])(c1_layer)\n",
    "    c2_layer = layers.RepeatVector(img_size * img_size)(c2)\n",
    "    c2_layer = layers.Reshape([img_size, img_size, 1])(c2_layer)\n",
    "    concat = layers.Concatenate(axis=-1)([ref_img, c1_layer, c2_layer])\n",
    "    return concat\n",
    "\n",
    "def shortcut(inp,filters,strides,kernel_size=(3,3),padding=\"same\"):\n",
    "    shortcut_conv = layers.Conv2D(filters, kernel_size=(1,1), padding=padding, strides=strides)(inp)\n",
    "    shortcut_bn = layers.BatchNormalization()(shortcut_conv)\n",
    "    return shortcut_bn\n",
    "    \n",
    "def residual_block(inp,filters,kernel_size=(3,3),padding=\"same\",strides=[1,1],act=True):\n",
    "    sc = shortcut(inp,filters,strides[0])\n",
    "    if act == True:\n",
    "        inp = bn_act_block(inp)\n",
    "    conv = conv_block(inp,filters,strides)\n",
    "    add = layers.Add()([conv, sc])\n",
    "    return add\n",
    "\n",
    "def upsample_and_concat(inp1,inp2):\n",
    "    upsample = layers.UpSampling2D((2, 2))(inp1)\n",
    "    concat = layers.Concatenate()([upsample,inp2])\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "enabling-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resUnet():\n",
    "    f = [16, 32, 64, 128, 256, 512]\n",
    "    img_size = 128\n",
    "    ref_img = layers.Input(shape=(img_size,img_size,1))\n",
    "    c1      = layers.Input(shape=(1,))\n",
    "    c2      = layers.Input(shape=(1,))\n",
    "    inp     = inp_concat(ref_img,c1,c2)\n",
    "    e       = residual_block(inp, f[0])\n",
    "    e1      = residual_block(e,f[1],act=False)\n",
    "    e2      = residual_block(e1,f[2],strides=[2,1])\n",
    "    e3      = residual_block(e2,f[3],strides=[2,1])\n",
    "    e4      = residual_block(e3,f[4],strides=[2,1])\n",
    "    bridge  = residual_block(e4,f[5],strides=[2,1])\n",
    "    us_c    = upsample_and_concat(bridge,e4)\n",
    "    d1      = residual_block(us_c,f[4])\n",
    "    us_c1   = upsample_and_concat(d1,e3)\n",
    "    d2      = residual_block(us_c1,f[3])\n",
    "    us_c2   = upsample_and_concat(d2,e2)\n",
    "    d3      = residual_block(us_c2,f[2])\n",
    "    us_c3   = upsample_and_concat(d3,e1)\n",
    "    d4      = residual_block(us_c3,f[1])\n",
    "    out     = layers.Conv2D(2,kernel_size=(1,1),activation='sigmoid')(d4)\n",
    "    return Model([ref_img,c1,c2],out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-motorcycle",
   "metadata": {},
   "source": [
    "# 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "living-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 불러오기 \n",
    "ref_path = '../data/reference_image/'\n",
    "lbl_path = '../data/binary_label_11/'\n",
    "# 학습 데이터 인덱스 불러오기\n",
    "train_ref_num = np.load('../TopOpNet/train_ref_num.npy')\n",
    "valid_ref_num = np.load('../TopOpNet/valid_ref_num.npy')\n",
    "test_ref_num = np.load('../TopOpNet/test_ref_num.npy')\n",
    "tdl = DataLoader(ref_path, lbl_path, n_batch=batch_size, ref_num_list=train_ref_num, pdf_npy=None, from_mem=True)\n",
    "vdl = DataLoader(ref_path, lbl_path, n_batch=batch_size, ref_num_list=valid_ref_num, pdf_npy=None, from_mem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "effective-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "batch_size = 64 \n",
    "model = resUnet()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \n",
    "trainable_variables = model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-august",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/50000 [00:00<5:56:16,  2.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m0\u001b[0m train_loss: \u001b[31m0.35808334\u001b[0m val_loss: \u001b[33m0.36208725\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 102/50000 [00:35<5:35:31,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m100\u001b[0m train_loss: \u001b[31m0.35891545\u001b[0m val_loss: \u001b[33m0.35776395\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 202/50000 [01:09<5:20:15,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m200\u001b[0m train_loss: \u001b[31m0.35422847\u001b[0m val_loss: \u001b[33m0.35174596\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 302/50000 [01:43<4:55:08,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: \u001b[34m300\u001b[0m train_loss: \u001b[31m0.35460374\u001b[0m val_loss: \u001b[33m0.35186976\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 361/50000 [02:03<4:41:28,  2.94it/s]"
     ]
    }
   ],
   "source": [
    "best_loss = 0.7639\n",
    "for iterate in tqdm(range(50000)):\n",
    "    ref_imgs, c1, c2, opt_imgs = tdl.get_next()\n",
    "    opt_image_one_hot = keras.backend.one_hot(opt_imgs, 2)\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model([ref_imgs.reshape(batch_size,128,128,1), c1, c2])\n",
    "        train_loss = tf.reduce_mean(keras.backend.categorical_crossentropy(opt_image_one_hot\n",
    "                                                                               ,logits,from_logits=True))\n",
    "    grads = tape.gradient(train_loss,trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "    \n",
    "    if iterate%100 == 0:\n",
    "        ref_imgs, c1, c2, opt_imgs = vdl.get_next()\n",
    "        opt_image_one_hot = keras.backend.one_hot(opt_imgs, 2)\n",
    "        logits = model([ref_imgs.reshape(batch_size,128,128,1), c1, c2])\n",
    "        val_loss = tf.reduce_mean(keras.backend.categorical_crossentropy(opt_image_one_hot\n",
    "                                                                        ,logits,from_logits=True))\n",
    "        \n",
    "        if best_loss > val_loss : \n",
    "            best_loss = val_loss\n",
    "            tf.keras.models.save_model(model, './20210123_baseline2_model.h5')\n",
    "            print(colored('model saved','cyan'))\n",
    "        \n",
    "        print('iteration:',colored(iterate,'blue')\n",
    "              ,'train_loss:',colored(np.array(train_loss),'red')\n",
    "              ,'val_loss:',colored(np.array(val_loss),'yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(image_grid_rows=4, image_grid_columns=4):\n",
    "    ref_image, c1, c2, opt_image = tdl.get_next()\n",
    "    gen_imgs = model.predict([ref_image, c1, c2])\n",
    "    gen_imgs = gen_imgs[1]\n",
    "    # 이미지 픽셀 값을 [0, 1] 사이로 스케일 조정\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    # 이미지 그리드 설정\n",
    "    fig, axs = plt.subplots(image_grid_rows,\n",
    "                            image_grid_columns,\n",
    "                            figsize=(4, 4),\n",
    "                            sharey=True,\n",
    "                            sharex=True)\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(image_grid_rows):\n",
    "        for j in range(image_grid_columns):\n",
    "            # 이미지 그리드 출력\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(\n",
    "    model, './20210123_baseline_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = tf.keras.models.load_model('./20210123_baseline_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def ds_load(ref_num):\n",
    "    lbl_names,ref_list, c1_list, c2_list = [], [], [] ,[]\n",
    "    ref_imgs, opt_imgs = [],[]\n",
    "    for i in range(len(ref_num)):\n",
    "        for lbl_name in os.listdir(lbl_path):\n",
    "            ref_from_lbl = lbl_name.split('_')[0]\n",
    "            if str(i) == ref_from_lbl:\n",
    "                lbl_names.append(lbl_name)\n",
    "                #이미지 불러오기\n",
    "                ref_img = cv2.imread(ref_path+'Binary_{}.jpg'.format(i),cv2.IMREAD_GRAYSCALE)\n",
    "                _,ref_img = cv2.threshold(ref_img,127,255,cv2.THRESH_BINARY_INV)\n",
    "                ref_imgs.append(ref_img/255.)\n",
    "                opt_img = cv2.imread(lbl_path+lbl_name,cv2.IMREAD_GRAYSCALE)\n",
    "                _,opt_img = cv2.threshold(opt_img,127,255,cv2.THRESH_BINARY_INV)\n",
    "                opt_imgs.append(opt_img/255.)\n",
    "                # c값 불러오기\n",
    "                lbl_name=lbl_name.split('.png')\n",
    "                lbl_name=lbl_name[0].split('_')\n",
    "                ref,c1,c2=lbl_name\n",
    "                ref_list.append(np.int(ref))\n",
    "                c1_list.append(np.float(c1))\n",
    "                c2_list.append(np.float(c2))\n",
    "    return np.asarray(ref_imgs),np.asarray(opt_imgs),\\\n",
    "            np.asarray(ref_list),np.asarray(c1_list),np.asarray(c2_list),\\\n",
    "             np.asarray(lbl_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(gen_imgs,opt_imgs,valid_ds,lbl_names):\n",
    "    ref_list, c1_list, c2_list = valid_ds\n",
    "    print(len(ref_list))\n",
    "    for i in range(len(ref_list)):\n",
    "        gen_img,opt_img = gen_imgs[i]*255,opt_imgs[i]*255\n",
    "        gen_img,opt_img = gen_img.astype(np.uint8),opt_img.astype(np.uint8)\n",
    "        gen_img,opt_img = Image.fromarray(gen_img),Image.fromarray(opt_img)\n",
    "\n",
    "        savename = lbl_names[i]\n",
    "        gen_img.save('./baseline2/validset/gen_imgs/'+savename)\n",
    "        opt_img.save('./baseline2/validset/gt_imgs/'+savename) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_imgs, opt_imgs, ref_list, c1_list, c2_list, lbl_names = ds_load(valid_ref_num) #valid set 전체 불러오기 \n",
    "prep_c1_list = np.log10(c1_list) # c1 log10으로 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "start,stop = 10000,len(ref_imgs)\n",
    "_, gen_imgs = trained_model.predict([ref_imgs[start:stop], prep_c1_list[start:stop], c2_list[start:stop]])\n",
    "valid_ds = ref_list[start:stop], c1_list[start:stop], c2_list[start:stop]\n",
    "save_result(gen_imgs,opt_imgs[start:stop],valid_ds,lbl_names[start:stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ref_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-transfer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
