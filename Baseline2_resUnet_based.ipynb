{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informal-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, BatchNormalization,Reshape,Conv2DTranspose\n",
    "from tensorflow.keras.layers import MaxPooling2D, Activation,Add,LeakyReLU,Flatten,Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from dataloader import get_ref_num_list, DataLoader\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_visible_devices(gpus[3], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-bracelet",
   "metadata": {},
   "source": [
    "# 모델 구축 (ResUnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "secret-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inp,filters,strides,kernel_size=(3,3),padding=\"same\"):\n",
    "    conv = layers.Conv2D(filters, kernel_size=kernel_size, padding=padding, strides=strides[0])(inp)\n",
    "    bn_act = bn_act_block(conv)\n",
    "    conv = layers.Conv2D(filters, kernel_size=kernel_size, padding=padding, strides=strides[1])(bn_act)\n",
    "    return conv \n",
    "\n",
    "def bn_act_block(inp):\n",
    "    bn = layers.BatchNormalization()(inp)\n",
    "    act = layers.Activation('relu')(bn)\n",
    "    return act\n",
    "\n",
    "def inp_concat(ref_img,c1,c2):\n",
    "    img_size = 128\n",
    "    c1_layer = layers.RepeatVector(img_size * img_size)(c1)\n",
    "    c1_layer = layers.Reshape([img_size, img_size, 1])(c1_layer)\n",
    "    c2_layer = layers.RepeatVector(img_size * img_size)(c2)\n",
    "    c2_layer = layers.Reshape([img_size, img_size, 1])(c2_layer)\n",
    "    concat = layers.Concatenate(axis=-1)([ref_img, c1_layer, c2_layer])\n",
    "    return concat\n",
    "\n",
    "def shortcut(inp,filters,strides,kernel_size=(3,3),padding=\"same\"):\n",
    "    shortcut_conv = layers.Conv2D(filters, kernel_size=(1,1), padding=padding, strides=strides)(inp)\n",
    "    shortcut_bn = layers.BatchNormalization()(shortcut_conv)\n",
    "    return shortcut_bn\n",
    "    \n",
    "def residual_block(inp,filters,kernel_size=(3,3),padding=\"same\",strides=[1,1],act=True):\n",
    "    sc = shortcut(inp,filters,strides[0])\n",
    "    if act == True:\n",
    "        inp = bn_act_block(inp)\n",
    "    conv = conv_block(inp,filters,strides)\n",
    "    add = layers.Add()([conv, sc])\n",
    "    return add\n",
    "\n",
    "def upsample_and_concat(inp1,inp2):\n",
    "    upsample = layers.UpSampling2D((2, 2))(inp1)\n",
    "    concat = layers.Concatenate()([upsample,inp2])\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mental-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resUnet():\n",
    "    f = [16, 32, 64, 128, 256, 512]\n",
    "    img_size = 128\n",
    "    ref_img = layers.Input(shape=(img_size,img_size,1))\n",
    "    c1      = layers.Input(shape=(1,))\n",
    "    c2      = layers.Input(shape=(1,))\n",
    "    inp     = inp_concat(ref_img,c1,c2)\n",
    "    e       = residual_block(inp, f[0])\n",
    "    e1      = residual_block(e,f[1],act=False)\n",
    "    e2      = residual_block(e1,f[2],strides=[2,1])\n",
    "    e3      = residual_block(e2,f[3],strides=[2,1])\n",
    "    e4      = residual_block(e3,f[4],strides=[2,1])\n",
    "    bridge  = residual_block(e4,f[5],strides=[2,1])\n",
    "    us_c    = upsample_and_concat(bridge,e4)\n",
    "    d1      = residual_block(us_c,f[4])\n",
    "    us_c1   = upsample_and_concat(d1,e3)\n",
    "    d2      = residual_block(us_c1,f[3])\n",
    "    us_c2   = upsample_and_concat(d2,e2)\n",
    "    d3      = residual_block(us_c2,f[2])\n",
    "    us_c3   = upsample_and_concat(d3,e1)\n",
    "    d4      = residual_block(us_c3,f[1])\n",
    "    out     = layers.Conv2D(2,kernel_size=(1,1),activation='sigmoid')(d4)\n",
    "    return Model([ref_img,c1,c2],out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-password",
   "metadata": {},
   "source": [
    "# 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moved-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 불러오기 \n",
    "ref_path = '../data/reference_image/'\n",
    "lbl_path = '../data/binary_label_11/'\n",
    "# 학습 데이터 인덱스 불러오기\n",
    "train_ref_num = np.load('../TopOpNet/train_ref_num.npy')\n",
    "valid_ref_num = np.load('../TopOpNet/valid_ref_num.npy')\n",
    "test_ref_num = np.load('../TopOpNet/test_ref_num.npy')\n",
    "tdl = DataLoader(ref_path, lbl_path, n_batch=batch_size, ref_num_list=train_ref_num, pdf_npy=None, from_mem=True)\n",
    "vdl = DataLoader(ref_path, lbl_path, n_batch=batch_size, ref_num_list=valid_ref_num, pdf_npy=None, from_mem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "operating-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "batch_size = 64 \n",
    "model = resUnet()\n",
    "loss = tf.keras.losses.categorical_crossentropy\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \n",
    "trainable_variables = model.trainable_variables\n",
    "model = resUnet()\n",
    "model.compile(loss=loss,optimizer=optimizer,metrics='acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-gambling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m0\u001b[0m train_loss: \u001b[31m0.7706\u001b[0m train_acc: \u001b[31m0.5459\u001b[0m % val_loss: \u001b[33m0.7536\u001b[0m val_acc: \u001b[33m0.565\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m100\u001b[0m train_loss: \u001b[31m0.1378\u001b[0m train_acc: \u001b[31m0.9465\u001b[0m % val_loss: \u001b[33m0.3597\u001b[0m val_acc: \u001b[33m0.8372\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m200\u001b[0m train_loss: \u001b[31m0.119\u001b[0m train_acc: \u001b[31m0.9503\u001b[0m % val_loss: \u001b[33m0.2561\u001b[0m val_acc: \u001b[33m0.8912\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m300\u001b[0m train_loss: \u001b[31m0.0928\u001b[0m train_acc: \u001b[31m0.9618\u001b[0m % val_loss: \u001b[33m0.1308\u001b[0m val_acc: \u001b[33m0.9472\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m400\u001b[0m train_loss: \u001b[31m0.0971\u001b[0m train_acc: \u001b[31m0.9579\u001b[0m % val_loss: \u001b[33m0.1014\u001b[0m val_acc: \u001b[33m0.9575\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m500\u001b[0m train_loss: \u001b[31m0.0855\u001b[0m train_acc: \u001b[31m0.9639\u001b[0m % val_loss: \u001b[33m0.0899\u001b[0m val_acc: \u001b[33m0.9625\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m600\u001b[0m train_loss: \u001b[31m0.0862\u001b[0m train_acc: \u001b[31m0.9631\u001b[0m % val_loss: \u001b[33m0.0819\u001b[0m val_acc: \u001b[33m0.9654\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m700\u001b[0m train_loss: \u001b[31m0.0868\u001b[0m train_acc: \u001b[31m0.9624\u001b[0m % val_loss: \u001b[33m0.0809\u001b[0m val_acc: \u001b[33m0.9659\u001b[0m %\n",
      "iteration: \u001b[34m800\u001b[0m train_loss: \u001b[31m0.0806\u001b[0m train_acc: \u001b[31m0.965\u001b[0m % val_loss: \u001b[33m0.0856\u001b[0m val_acc: \u001b[33m0.9628\u001b[0m %\n",
      "iteration: \u001b[34m900\u001b[0m train_loss: \u001b[31m0.0731\u001b[0m train_acc: \u001b[31m0.9683\u001b[0m % val_loss: \u001b[33m0.0824\u001b[0m val_acc: \u001b[33m0.9642\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m1000\u001b[0m train_loss: \u001b[31m0.086\u001b[0m train_acc: \u001b[31m0.9628\u001b[0m % val_loss: \u001b[33m0.0761\u001b[0m val_acc: \u001b[33m0.9673\u001b[0m %\n",
      "iteration: \u001b[34m1100\u001b[0m train_loss: \u001b[31m0.0712\u001b[0m train_acc: \u001b[31m0.9687\u001b[0m % val_loss: \u001b[33m0.0799\u001b[0m val_acc: \u001b[33m0.9656\u001b[0m %\n",
      "iteration: \u001b[34m1200\u001b[0m train_loss: \u001b[31m0.068\u001b[0m train_acc: \u001b[31m0.9702\u001b[0m % val_loss: \u001b[33m0.079\u001b[0m val_acc: \u001b[33m0.9652\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m1300\u001b[0m train_loss: \u001b[31m0.0777\u001b[0m train_acc: \u001b[31m0.9658\u001b[0m % val_loss: \u001b[33m0.0701\u001b[0m val_acc: \u001b[33m0.9694\u001b[0m %\n",
      "iteration: \u001b[34m1400\u001b[0m train_loss: \u001b[31m0.0715\u001b[0m train_acc: \u001b[31m0.9684\u001b[0m % val_loss: \u001b[33m0.0796\u001b[0m val_acc: \u001b[33m0.9654\u001b[0m %\n",
      "iteration: \u001b[34m1500\u001b[0m train_loss: \u001b[31m0.0738\u001b[0m train_acc: \u001b[31m0.9674\u001b[0m % val_loss: \u001b[33m0.0723\u001b[0m val_acc: \u001b[33m0.9686\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m1600\u001b[0m train_loss: \u001b[31m0.0655\u001b[0m train_acc: \u001b[31m0.9716\u001b[0m % val_loss: \u001b[33m0.0652\u001b[0m val_acc: \u001b[33m0.9713\u001b[0m %\n",
      "iteration: \u001b[34m1700\u001b[0m train_loss: \u001b[31m0.0728\u001b[0m train_acc: \u001b[31m0.9684\u001b[0m % val_loss: \u001b[33m0.0719\u001b[0m val_acc: \u001b[33m0.9676\u001b[0m %\n",
      "iteration: \u001b[34m1800\u001b[0m train_loss: \u001b[31m0.0735\u001b[0m train_acc: \u001b[31m0.9672\u001b[0m % val_loss: \u001b[33m0.0697\u001b[0m val_acc: \u001b[33m0.9691\u001b[0m %\n",
      "iteration: \u001b[34m1900\u001b[0m train_loss: \u001b[31m0.0631\u001b[0m train_acc: \u001b[31m0.9722\u001b[0m % val_loss: \u001b[33m0.0716\u001b[0m val_acc: \u001b[33m0.9686\u001b[0m %\n",
      "iteration: \u001b[34m2000\u001b[0m train_loss: \u001b[31m0.0716\u001b[0m train_acc: \u001b[31m0.9688\u001b[0m % val_loss: \u001b[33m0.0712\u001b[0m val_acc: \u001b[33m0.9688\u001b[0m %\n",
      "iteration: \u001b[34m2100\u001b[0m train_loss: \u001b[31m0.0773\u001b[0m train_acc: \u001b[31m0.9658\u001b[0m % val_loss: \u001b[33m0.0676\u001b[0m val_acc: \u001b[33m0.9704\u001b[0m %\n",
      "iteration: \u001b[34m2200\u001b[0m train_loss: \u001b[31m0.0686\u001b[0m train_acc: \u001b[31m0.9699\u001b[0m % val_loss: \u001b[33m0.073\u001b[0m val_acc: \u001b[33m0.9688\u001b[0m %\n",
      "iteration: \u001b[34m2300\u001b[0m train_loss: \u001b[31m0.0637\u001b[0m train_acc: \u001b[31m0.9719\u001b[0m % val_loss: \u001b[33m0.069\u001b[0m val_acc: \u001b[33m0.9694\u001b[0m %\n",
      "iteration: \u001b[34m2400\u001b[0m train_loss: \u001b[31m0.0631\u001b[0m train_acc: \u001b[31m0.9725\u001b[0m % val_loss: \u001b[33m0.0665\u001b[0m val_acc: \u001b[33m0.9709\u001b[0m %\n",
      "iteration: \u001b[34m2500\u001b[0m train_loss: \u001b[31m0.0658\u001b[0m train_acc: \u001b[31m0.9706\u001b[0m % val_loss: \u001b[33m0.0754\u001b[0m val_acc: \u001b[33m0.9665\u001b[0m %\n",
      "iteration: \u001b[34m2600\u001b[0m train_loss: \u001b[31m0.0641\u001b[0m train_acc: \u001b[31m0.9719\u001b[0m % val_loss: \u001b[33m0.0685\u001b[0m val_acc: \u001b[33m0.9696\u001b[0m %\n",
      "iteration: \u001b[34m2700\u001b[0m train_loss: \u001b[31m0.0722\u001b[0m train_acc: \u001b[31m0.9679\u001b[0m % val_loss: \u001b[33m0.0718\u001b[0m val_acc: \u001b[33m0.9687\u001b[0m %\n",
      "iteration: \u001b[34m2800\u001b[0m train_loss: \u001b[31m0.067\u001b[0m train_acc: \u001b[31m0.97\u001b[0m % val_loss: \u001b[33m0.0657\u001b[0m val_acc: \u001b[33m0.9712\u001b[0m %\n",
      "iteration: \u001b[34m2900\u001b[0m train_loss: \u001b[31m0.0699\u001b[0m train_acc: \u001b[31m0.9687\u001b[0m % val_loss: \u001b[33m0.0663\u001b[0m val_acc: \u001b[33m0.9713\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m3000\u001b[0m train_loss: \u001b[31m0.0643\u001b[0m train_acc: \u001b[31m0.972\u001b[0m % val_loss: \u001b[33m0.0647\u001b[0m val_acc: \u001b[33m0.9712\u001b[0m %\n",
      "iteration: \u001b[34m3100\u001b[0m train_loss: \u001b[31m0.0706\u001b[0m train_acc: \u001b[31m0.9685\u001b[0m % val_loss: \u001b[33m0.0677\u001b[0m val_acc: \u001b[33m0.9706\u001b[0m %\n",
      "iteration: \u001b[34m3200\u001b[0m train_loss: \u001b[31m0.0597\u001b[0m train_acc: \u001b[31m0.9738\u001b[0m % val_loss: \u001b[33m0.0666\u001b[0m val_acc: \u001b[33m0.9701\u001b[0m %\n",
      "iteration: \u001b[34m3300\u001b[0m train_loss: \u001b[31m0.061\u001b[0m train_acc: \u001b[31m0.9729\u001b[0m % val_loss: \u001b[33m0.0684\u001b[0m val_acc: \u001b[33m0.9696\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m3400\u001b[0m train_loss: \u001b[31m0.0647\u001b[0m train_acc: \u001b[31m0.9715\u001b[0m % val_loss: \u001b[33m0.0642\u001b[0m val_acc: \u001b[33m0.9713\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m3500\u001b[0m train_loss: \u001b[31m0.0638\u001b[0m train_acc: \u001b[31m0.9712\u001b[0m % val_loss: \u001b[33m0.0606\u001b[0m val_acc: \u001b[33m0.9736\u001b[0m %\n",
      "iteration: \u001b[34m3600\u001b[0m train_loss: \u001b[31m0.0717\u001b[0m train_acc: \u001b[31m0.9683\u001b[0m % val_loss: \u001b[33m0.0713\u001b[0m val_acc: \u001b[33m0.9681\u001b[0m %\n",
      "iteration: \u001b[34m3700\u001b[0m train_loss: \u001b[31m0.06\u001b[0m train_acc: \u001b[31m0.9733\u001b[0m % val_loss: \u001b[33m0.0629\u001b[0m val_acc: \u001b[33m0.9727\u001b[0m %\n",
      "iteration: \u001b[34m3800\u001b[0m train_loss: \u001b[31m0.0658\u001b[0m train_acc: \u001b[31m0.9707\u001b[0m % val_loss: \u001b[33m0.067\u001b[0m val_acc: \u001b[33m0.9704\u001b[0m %\n",
      "iteration: \u001b[34m3900\u001b[0m train_loss: \u001b[31m0.0649\u001b[0m train_acc: \u001b[31m0.971\u001b[0m % val_loss: \u001b[33m0.061\u001b[0m val_acc: \u001b[33m0.9733\u001b[0m %\n",
      "iteration: \u001b[34m4000\u001b[0m train_loss: \u001b[31m0.062\u001b[0m train_acc: \u001b[31m0.9721\u001b[0m % val_loss: \u001b[33m0.0641\u001b[0m val_acc: \u001b[33m0.9717\u001b[0m %\n",
      "iteration: \u001b[34m4100\u001b[0m train_loss: \u001b[31m0.0682\u001b[0m train_acc: \u001b[31m0.9701\u001b[0m % val_loss: \u001b[33m0.0672\u001b[0m val_acc: \u001b[33m0.9706\u001b[0m %\n",
      "iteration: \u001b[34m4200\u001b[0m train_loss: \u001b[31m0.067\u001b[0m train_acc: \u001b[31m0.9701\u001b[0m % val_loss: \u001b[33m0.0655\u001b[0m val_acc: \u001b[33m0.9708\u001b[0m %\n",
      "iteration: \u001b[34m4300\u001b[0m train_loss: \u001b[31m0.0671\u001b[0m train_acc: \u001b[31m0.9697\u001b[0m % val_loss: \u001b[33m0.0614\u001b[0m val_acc: \u001b[33m0.9736\u001b[0m %\n",
      "iteration: \u001b[34m4400\u001b[0m train_loss: \u001b[31m0.0621\u001b[0m train_acc: \u001b[31m0.9727\u001b[0m % val_loss: \u001b[33m0.0664\u001b[0m val_acc: \u001b[33m0.9706\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m4500\u001b[0m train_loss: \u001b[31m0.0569\u001b[0m train_acc: \u001b[31m0.9745\u001b[0m % val_loss: \u001b[33m0.0604\u001b[0m val_acc: \u001b[33m0.973\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m4600\u001b[0m train_loss: \u001b[31m0.0607\u001b[0m train_acc: \u001b[31m0.9732\u001b[0m % val_loss: \u001b[33m0.0598\u001b[0m val_acc: \u001b[33m0.9743\u001b[0m %\n",
      "iteration: \u001b[34m4700\u001b[0m train_loss: \u001b[31m0.0596\u001b[0m train_acc: \u001b[31m0.9734\u001b[0m % val_loss: \u001b[33m0.0721\u001b[0m val_acc: \u001b[33m0.9676\u001b[0m %\n",
      "iteration: \u001b[34m4800\u001b[0m train_loss: \u001b[31m0.0648\u001b[0m train_acc: \u001b[31m0.9712\u001b[0m % val_loss: \u001b[33m0.0617\u001b[0m val_acc: \u001b[33m0.9729\u001b[0m %\n",
      "iteration: \u001b[34m4900\u001b[0m train_loss: \u001b[31m0.0622\u001b[0m train_acc: \u001b[31m0.9726\u001b[0m % val_loss: \u001b[33m0.0635\u001b[0m val_acc: \u001b[33m0.9719\u001b[0m %\n",
      "iteration: \u001b[34m5000\u001b[0m train_loss: \u001b[31m0.061\u001b[0m train_acc: \u001b[31m0.9729\u001b[0m % val_loss: \u001b[33m0.0622\u001b[0m val_acc: \u001b[33m0.9728\u001b[0m %\n",
      "iteration: \u001b[34m5100\u001b[0m train_loss: \u001b[31m0.0579\u001b[0m train_acc: \u001b[31m0.9744\u001b[0m % val_loss: \u001b[33m0.0628\u001b[0m val_acc: \u001b[33m0.9726\u001b[0m %\n",
      "iteration: \u001b[34m5200\u001b[0m train_loss: \u001b[31m0.0717\u001b[0m train_acc: \u001b[31m0.9677\u001b[0m % val_loss: \u001b[33m0.06\u001b[0m val_acc: \u001b[33m0.9739\u001b[0m %\n",
      "iteration: \u001b[34m5300\u001b[0m train_loss: \u001b[31m0.0564\u001b[0m train_acc: \u001b[31m0.9751\u001b[0m % val_loss: \u001b[33m0.0598\u001b[0m val_acc: \u001b[33m0.9739\u001b[0m %\n",
      "\u001b[36mmodel saved\u001b[0m\n",
      "iteration: \u001b[34m5400\u001b[0m train_loss: \u001b[31m0.0606\u001b[0m train_acc: \u001b[31m0.9725\u001b[0m % val_loss: \u001b[33m0.0566\u001b[0m val_acc: \u001b[33m0.9751\u001b[0m %\n"
     ]
    }
   ],
   "source": [
    "best_loss = 1\n",
    "for iterate in range(50000):\n",
    "    ref_imgs, c1, c2, opt_imgs = tdl.get_next()\n",
    "    train_loss,acc = model.train_on_batch([ref_imgs,c1,c2],keras.backend.one_hot(opt_imgs, 2))\n",
    "    if iterate%100 ==0:\n",
    "        ref_imgs, c1, c2, opt_imgs = vdl.get_next()\n",
    "        val_loss,val_acc = model.test_on_batch([ref_imgs, c1, c2],keras.backend.one_hot(opt_imgs, 2))\n",
    "    \n",
    "        if best_loss > val_loss : \n",
    "            best_loss = val_loss\n",
    "            tf.keras.models.save_model(model, './20210123_baseline2_model.h5')\n",
    "            print(colored('model saved','cyan'))\n",
    "\n",
    "        print('iteration:',colored(iterate,'blue')\n",
    "             ,'train_loss:',colored(np.round(train_loss,4),'red'),'train_acc:',colored(np.round(acc,4),'red'),'%'\n",
    "             ,'val_loss:',colored(np.round(val_loss,4),'yellow'),'val_acc:',colored(np.round(val_acc,4),'yellow'),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(image_grid_rows=4, image_grid_columns=4):\n",
    "    ref_image, c1, c2, opt_image = tdl.get_next()\n",
    "    gen_imgs = model.predict([ref_image, c1, c2])\n",
    "    gen_imgs = gen_imgs[1]\n",
    "    # 이미지 픽셀 값을 [0, 1] 사이로 스케일 조정\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    # 이미지 그리드 설정\n",
    "    fig, axs = plt.subplots(image_grid_rows,\n",
    "                            image_grid_columns,\n",
    "                            figsize=(4, 4),\n",
    "                            sharey=True,\n",
    "                            sharex=True)\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(image_grid_rows):\n",
    "        for j in range(image_grid_columns):\n",
    "            # 이미지 그리드 출력\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = tf.keras.models.load_model('./20210123_baseline_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def ds_load(ref_num):\n",
    "    lbl_names,ref_list, c1_list, c2_list = [], [], [] ,[]\n",
    "    ref_imgs, opt_imgs = [],[]\n",
    "    for i in range(len(ref_num)):\n",
    "        for lbl_name in os.listdir(lbl_path):\n",
    "            ref_from_lbl = lbl_name.split('_')[0]\n",
    "            if str(i) == ref_from_lbl:\n",
    "                lbl_names.append(lbl_name)\n",
    "                #이미지 불러오기\n",
    "                ref_img = cv2.imread(ref_path+'Binary_{}.jpg'.format(i),cv2.IMREAD_GRAYSCALE)\n",
    "                _,ref_img = cv2.threshold(ref_img,127,255,cv2.THRESH_BINARY_INV)\n",
    "                ref_imgs.append(ref_img/255.)\n",
    "                opt_img = cv2.imread(lbl_path+lbl_name,cv2.IMREAD_GRAYSCALE)\n",
    "                _,opt_img = cv2.threshold(opt_img,127,255,cv2.THRESH_BINARY_INV)\n",
    "                opt_imgs.append(opt_img/255.)\n",
    "                # c값 불러오기\n",
    "                lbl_name=lbl_name.split('.png')\n",
    "                lbl_name=lbl_name[0].split('_')\n",
    "                ref,c1,c2=lbl_name\n",
    "                ref_list.append(np.int(ref))\n",
    "                c1_list.append(np.float(c1))\n",
    "                c2_list.append(np.float(c2))\n",
    "    return np.asarray(ref_imgs),np.asarray(opt_imgs),\\\n",
    "            np.asarray(ref_list),np.asarray(c1_list),np.asarray(c2_list),\\\n",
    "             np.asarray(lbl_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(gen_imgs,opt_imgs,valid_ds,lbl_names):\n",
    "    ref_list, c1_list, c2_list = valid_ds\n",
    "    print(len(ref_list))\n",
    "    for i in range(len(ref_list)):\n",
    "        gen_img,opt_img = gen_imgs[i]*255,opt_imgs[i]*255\n",
    "        gen_img,opt_img = gen_img.astype(np.uint8),opt_img.astype(np.uint8)\n",
    "        gen_img,opt_img = Image.fromarray(gen_img),Image.fromarray(opt_img)\n",
    "\n",
    "        savename = lbl_names[i]\n",
    "        gen_img.save('./baseline2/validset/gen_imgs/'+savename)\n",
    "        opt_img.save('./baseline2/validset/gt_imgs/'+savename) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_imgs, opt_imgs, ref_list, c1_list, c2_list, lbl_names = ds_load(valid_ref_num) #valid set 전체 불러오기 \n",
    "prep_c1_list = np.log10(c1_list) # c1 log10으로 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "start,stop = 10000,len(ref_imgs)\n",
    "_, gen_imgs = trained_model.predict([ref_imgs[start:stop], prep_c1_list[start:stop], c2_list[start:stop]])\n",
    "valid_ds = ref_list[start:stop], c1_list[start:stop], c2_list[start:stop]\n",
    "save_result(gen_imgs,opt_imgs[start:stop],valid_ds,lbl_names[start:stop])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
